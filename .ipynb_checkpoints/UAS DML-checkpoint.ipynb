{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7d8cc0-37a8-4995-9d6e-57dfabecb7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.1002_cssc.202201821</td>\n",
       "      <td>https://doi.org/10.5281/zenodo.7074790</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.1002_ece3.4466</td>\n",
       "      <td>https://doi.org/10.5061/dryad.r6nq870</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.1002_ece3.5260</td>\n",
       "      <td>https://doi.org/10.5061/dryad.2f62927</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>https://doi.org/10.5061/dryad.zw3r22854</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id              article_id                               dataset_id  \\\n",
       "0       0    10.1002_2017jc013030           https://doi.org/10.17882/49388   \n",
       "1       1  10.1002_cssc.202201821   https://doi.org/10.5281/zenodo.7074790   \n",
       "2       2       10.1002_ece3.4466    https://doi.org/10.5061/dryad.r6nq870   \n",
       "3       3       10.1002_ece3.5260    https://doi.org/10.5061/dryad.2f62927   \n",
       "4       4       10.1002_ece3.6144  https://doi.org/10.5061/dryad.zw3r22854   \n",
       "\n",
       "      type  \n",
       "0  Primary  \n",
       "1  Primary  \n",
       "2  Primary  \n",
       "3  Primary  \n",
       "4  Primary  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_pd = 'sample_submission.csv'\n",
    "pd_df = pd.read_csv(file_pd)\n",
    "\n",
    "\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14a31fa-72f6-4f40-8ca8-ec9e41c3445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parsing selesai. Data disimpan ke parsed_articles_train_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_from_element(elem, skip_tags=(\"xref\", \"table-wrap\", \"fig\", \"table\", \"label\")):\n",
    "    \"\"\"Ekstrak teks dari elemen XML, abaikan tag tertentu\"\"\"\n",
    "    texts = []\n",
    "    for sub in elem.iter():\n",
    "        if sub.tag not in skip_tags:\n",
    "            if sub.text:\n",
    "                texts.append(sub.text.strip())\n",
    "            if sub.tail:\n",
    "                texts.append(sub.tail.strip())\n",
    "    return \" \".join(texts)\n",
    "\n",
    "def parse_article_xml_clean(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Ambil article_id dari DOI\n",
    "        doi_elem = root.find(\".//article-id[@pub-id-type='doi']\")\n",
    "        article_id = doi_elem.text.strip().replace(\"/\", \"_\") if doi_elem is not None else os.path.basename(xml_path).replace(\".xml\", \"\")\n",
    "\n",
    "        # Ambil abstract\n",
    "        abstract_elem = root.find(\".//abstract\")\n",
    "        abstract = extract_text_from_element(abstract_elem) if abstract_elem is not None else \"\"\n",
    "\n",
    "        # Ambil body (tanpa xref, figure, table)\n",
    "        body_elem = root.find(\".//body\")\n",
    "        sections = []\n",
    "        if body_elem is not None:\n",
    "            for sec in body_elem.findall(\".//sec\"):\n",
    "                sec_text = extract_text_from_element(sec)\n",
    "                if sec_text:\n",
    "                    sections.append(sec_text)\n",
    "        body = \"\\n\\n\".join(sections)\n",
    "\n",
    "        return {\n",
    "            \"article_id\": article_id,\n",
    "            \"abstract\": abstract,\n",
    "            \"body\": body,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error in {xml_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 🔧 Sesuaikan path ini jika perlu\n",
    "xml_folder = \"train/XML\"\n",
    "\n",
    "# Proses semua file .xml\n",
    "parsed_articles = []\n",
    "for fname in os.listdir(xml_folder):\n",
    "    if fname.endswith(\".xml\"):\n",
    "        full_path = os.path.join(xml_folder, fname)\n",
    "        article_data = parse_article_xml_clean(full_path)\n",
    "        if article_data:\n",
    "            parsed_articles.append(article_data)\n",
    "\n",
    "# Simpan ke CSV yang rapi\n",
    "df_xml = pd.DataFrame(parsed_articles)\n",
    "df_xml.to_csv(\"parsed_articles_train_clean.csv\", index=False)\n",
    "print(\"✅ Parsing selesai. Data disimpan ke parsed_articles_train_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e161c2cc-3b96-4d32-9671-c98145d1bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Selesai! Semua teks dihapus. File: train_merged_no_text.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load hasil parsing artikel\n",
    "df_articles = pd.read_csv(\"parsed_articles_train_clean.csv\")\n",
    "\n",
    "# 2. Load label (relasi article_id ke dataset_id dan type)\n",
    "df_labels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "# 3. Gabungkan ke satu DataFrame berdasarkan article_id\n",
    "df_merged = df_labels.merge(df_articles, on=\"article_id\", how=\"left\")\n",
    "\n",
    "# 4. Drop kolom abstract, body, dan text jika ada\n",
    "df_merged = df_merged.drop(columns=[\"abstract\", \"body\"], errors=\"ignore\")\n",
    "if \"text\" in df_merged.columns:\n",
    "    df_merged = df_merged.drop(columns=[\"text\"])\n",
    "\n",
    "# 5. Simpan ke file CSV baru\n",
    "df_merged.to_csv(\"train_merged_no_text.csv\", index=False)\n",
    "\n",
    "print(\"✅ Selesai! Semua teks dihapus. File: train_merged_no_text.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a849fbd-1621-4801-a120-d3757d2d4352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parsing selesai. Data disimpan ke parsed_articles_test_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_from_element(elem, skip_tags=(\"xref\", \"table-wrap\", \"fig\", \"table\", \"label\")):\n",
    "    \"\"\"Ekstrak teks dari elemen XML, abaikan tag tertentu\"\"\"\n",
    "    texts = []\n",
    "    for sub in elem.iter():\n",
    "        if sub.tag not in skip_tags:\n",
    "            if sub.text:\n",
    "                texts.append(sub.text.strip())\n",
    "            if sub.tail:\n",
    "                texts.append(sub.tail.strip())\n",
    "    return \" \".join(texts)\n",
    "\n",
    "def parse_article_xml_clean(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Ambil article_id dari DOI\n",
    "        doi_elem = root.find(\".//article-id[@pub-id-type='doi']\")\n",
    "        article_id = doi_elem.text.strip().replace(\"/\", \"_\") if doi_elem is not None else os.path.basename(xml_path).replace(\".xml\", \"\")\n",
    "\n",
    "        # Ambil abstract\n",
    "        abstract_elem = root.find(\".//abstract\")\n",
    "        abstract = extract_text_from_element(abstract_elem) if abstract_elem is not None else \"\"\n",
    "\n",
    "        # Ambil body (tanpa xref, figure, table)\n",
    "        body_elem = root.find(\".//body\")\n",
    "        sections = []\n",
    "        if body_elem is not None:\n",
    "            for sec in body_elem.findall(\".//sec\"):\n",
    "                sec_text = extract_text_from_element(sec)\n",
    "                if sec_text:\n",
    "                    sections.append(sec_text)\n",
    "        body = \"\\n\\n\".join(sections)\n",
    "\n",
    "        return {\n",
    "            \"article_id\": article_id,\n",
    "            \"abstract\": abstract,\n",
    "            \"body\": body,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error in {xml_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 🔧 Sesuaikan path ini jika perlu\n",
    "xml_folder = \"test/XML\"\n",
    "\n",
    "# Proses semua file .xml\n",
    "parsed_articles = []\n",
    "for fname in os.listdir(xml_folder):\n",
    "    if fname.endswith(\".xml\"):\n",
    "        full_path = os.path.join(xml_folder, fname)\n",
    "        article_data = parse_article_xml_clean(full_path)\n",
    "        if article_data:\n",
    "            parsed_articles.append(article_data)\n",
    "\n",
    "# Simpan ke CSV yang rapi\n",
    "df_xml = pd.DataFrame(parsed_articles)\n",
    "df_xml.to_csv(\"parsed_articles_test_clean.csv\", index=False)\n",
    "print(\"✅ Parsing selesai. Data disimpan ke parsed_articles_test_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eeb1426-a114-4e68-b2c7-8c14a215877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Selesai! Semua teks dihapus. File: test_merged_no_text.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load hasil parsing artikel\n",
    "df_articles = pd.read_csv(\"parsed_articles_test_clean.csv\")\n",
    "\n",
    "# 2. Load label (relasi article_id ke dataset_id dan type)\n",
    "df_labels = pd.read_csv(\"train_merged_no_text.csv\")\n",
    "\n",
    "# 3. Gabungkan ke satu DataFrame berdasarkan article_id\n",
    "df_merged = df_labels.merge(df_articles, on=\"article_id\", how=\"left\")\n",
    "\n",
    "# 4. Drop kolom abstract, body, dan text jika ada\n",
    "df_merged = df_merged.drop(columns=[\"abstract\", \"body\"], errors=\"ignore\")\n",
    "if \"text\" in df_merged.columns:\n",
    "    df_merged = df_merged.drop(columns=[\"text\"])\n",
    "\n",
    "# 5. Simpan ke file CSV baru\n",
    "df_merged.to_csv(\"test_merged_no_text.csv\", index=False)\n",
    "\n",
    "print(\"✅ Selesai! Semua teks dihapus. File: test_merged_no_text.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54376cd1-f421-421c-955c-164530851609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribusi awal label:\n",
      "label\n",
      "1    449\n",
      "2    309\n",
      "0    270\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribusi setelah sampling:\n",
      "label\n",
      "1    449\n",
      "0    270\n",
      "2     31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total data setelah filter: 750\n",
      "\n",
      "🔄 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kahfi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Akurasi validasi: 0.7667\n",
      "\n",
      "Jumlah data test: 83\n",
      "⚠ Kolom type tidak ditemukan di test_labels.\n",
      "\n",
      "✅ File submission_logreg_balanced.csv berhasil dibuat!\n",
      "   row_id              article_id  \\\n",
      "0       1    10.1002_2017jc013030   \n",
      "1       2  10.1002_anie.201916483   \n",
      "2       3  10.1002_anie.202005531   \n",
      "3       4  10.1002_anie.202007717   \n",
      "4       5  10.1002_chem.201902131   \n",
      "\n",
      "                                          dataset_id       type  \n",
      "0                                            unknown    Primary  \n",
      "1                                            unknown  Secondary  \n",
      "2                                            unknown    Primary  \n",
      "3  https://www.ccdc.cam.ac.uk/services/structures...  Secondary  \n",
      "4                                            unknown    Primary  \n",
      "\n",
      "✅ Model disimpan sebagai text_classification_model_balanced.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# ==== 1. Load Data Train ====\n",
    "df_articles = pd.read_csv(\"parsed_articles_train_clean.csv\")\n",
    "df_labels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "# Gabungkan artikel dengan label\n",
    "df_train = df_labels.merge(df_articles, on=\"article_id\", how=\"left\")\n",
    "df_train[\"full_text\"] = (df_train[\"abstract\"].fillna(\"\") + \" \" + df_train[\"body\"].fillna(\"\"))\n",
    "\n",
    "# Mapping label ke angka (Primary=0, Secondary=1, Missing=2)\n",
    "label_map = {\"Primary\": 0, \"Secondary\": 1, \"Missing\": 2}\n",
    "df_train[\"label\"] = df_train[\"type\"].map(label_map)\n",
    "\n",
    "print(\"\\nDistribusi awal label:\")\n",
    "print(df_train[\"label\"].value_counts())\n",
    "\n",
    "# ======================================\n",
    "# 2. Kurangi proporsi Missing (sampling)\n",
    "# ======================================\n",
    "missing_df = df_train[df_train[\"label\"] == 2]\n",
    "non_missing_df = df_train[df_train[\"label\"] != 2]\n",
    "\n",
    "# Ambil hanya 10% dari Missing\n",
    "missing_df_sample = missing_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Gabungkan kembali\n",
    "df_balanced = pd.concat([non_missing_df, missing_df_sample])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nDistribusi setelah sampling:\")\n",
    "print(df_balanced[\"label\"].value_counts())\n",
    "\n",
    "# Fitur dan target\n",
    "X_text = df_balanced[\"full_text\"]\n",
    "y = df_balanced[\"type\"]\n",
    "\n",
    "print(f\"\\nTotal data setelah filter: {len(df_balanced)}\")\n",
    "\n",
    "# ==== 3. Split Data ====\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_text, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==== 4. Pipeline TF-IDF + Logistic Regression ====\n",
    "model_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=30000, ngram_range=(1,2))),\n",
    "    ('clf', LogisticRegression(max_iter=200, solver='saga'))\n",
    "])\n",
    "\n",
    "print(\"\\n🔄 Training model...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluasi validasi\n",
    "y_pred = model_pipeline.predict(X_val)\n",
    "print(f\"✅ Akurasi validasi: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "# ==== 5. Load Data Test ====\n",
    "df_test = pd.read_csv(\"parsed_articles_test_clean.csv\")\n",
    "df_test[\"full_text\"] = (df_test[\"abstract\"].fillna(\"\") + \" \" + df_test[\"body\"].fillna(\"\"))\n",
    "\n",
    "# Gabungkan dengan test_labels supaya dapat dataset_id dan type (kalau ada)\n",
    "test_labels = pd.read_csv(\"test_labels.csv\")\n",
    "df_test = df_test.merge(test_labels, on=\"article_id\", how=\"left\")\n",
    "\n",
    "print(\"\\nJumlah data test:\", len(df_test))\n",
    "\n",
    "# ==== 6. Evaluasi dengan 10% data test yang punya type ====\n",
    "if \"type\" in df_test.columns:\n",
    "    labeled_test = df_test.dropna(subset=[\"type\"])\n",
    "    print(f\"Ada {len(labeled_test)} data test yang punya label type.\")\n",
    "    \n",
    "    if len(labeled_test) > 0:\n",
    "        eval_sample = labeled_test.sample(frac=0.1, random_state=42)\n",
    "        eval_pred = model_pipeline.predict(eval_sample[\"full_text\"])\n",
    "        print(f\"✅ Akurasi pada 10% data test berlabel: {accuracy_score(eval_sample['type'], eval_pred):.4f}\")\n",
    "    else:\n",
    "        print(\"⚠ Tidak ada data test berlabel untuk evaluasi.\")\n",
    "else:\n",
    "    print(\"⚠ Kolom type tidak ditemukan di test_labels.\")\n",
    "\n",
    "# ==== 7. Prediksi seluruh test untuk submission ====\n",
    "test_predictions = model_pipeline.predict(df_test[\"full_text\"])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": range(1, len(df_test) + 1),\n",
    "    \"article_id\": df_test[\"article_id\"],\n",
    "    \"dataset_id\": df_test[\"dataset_id\"],  # dari test_labels\n",
    "    \"type\": test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_logreg_balanced.csv\", index=False)\n",
    "print(\"\\n✅ File submission_logreg_balanced.csv berhasil dibuat!\")\n",
    "print(submission.head())\n",
    "\n",
    "# ==== 8. Simpan Model ====\n",
    "joblib.dump(model_pipeline, \"text_classification_model_balanced.pkl\")\n",
    "print(\"\\n✅ Model disimpan sebagai text_classification_model_balanced.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a851afa2-1b21-4c2f-afff-df888e958d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribusi awal label:\n",
      "label\n",
      "1    449\n",
      "2    309\n",
      "0    270\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribusi setelah sampling:\n",
      "label\n",
      "1    449\n",
      "0    270\n",
      "2     31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total data setelah filter: 750\n",
      "\n",
      "🔄 Training XGBoost...\n",
      "[0]\tValidation-mlogloss:1.01250\n",
      "[50]\tValidation-mlogloss:0.44801\n",
      "[100]\tValidation-mlogloss:0.45012\n",
      "[150]\tValidation-mlogloss:0.45963\n",
      "[199]\tValidation-mlogloss:0.46384\n",
      "✅ Akurasi validasi: 0.7733\n",
      "\n",
      "Jumlah data test: 83\n",
      "⚠ Kolom type tidak ditemukan di test_labels.\n",
      "\n",
      "✅ File submission_xgboost_balanced.csv berhasil dibuat!\n",
      "   row_id              article_id  \\\n",
      "0       1    10.1002_2017jc013030   \n",
      "1       2  10.1002_anie.201916483   \n",
      "2       3  10.1002_anie.202005531   \n",
      "3       4  10.1002_anie.202007717   \n",
      "4       5  10.1002_chem.201902131   \n",
      "\n",
      "                                          dataset_id     type  \n",
      "0                                            unknown  Primary  \n",
      "1                                            unknown  Primary  \n",
      "2                                            unknown  Primary  \n",
      "3  https://www.ccdc.cam.ac.uk/services/structures...  Primary  \n",
      "4                                            unknown  Primary  \n",
      "\n",
      "✅ Model disimpan sebagai xgboost_text_model_balanced.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# ==== 1. Load Data Train ====\n",
    "df_articles = pd.read_csv(\"parsed_articles_train_clean.csv\")\n",
    "df_labels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "# Gabungkan artikel dengan label\n",
    "df_train = df_labels.merge(df_articles, on=\"article_id\", how=\"left\")\n",
    "df_train[\"full_text\"] = (df_train[\"abstract\"].fillna(\"\") + \" \" + df_train[\"body\"].fillna(\"\"))\n",
    "\n",
    "# Mapping label ke angka (Primary=0, Secondary=1, Missing=2)\n",
    "label_map = {\"Primary\": 0, \"Secondary\": 1, \"Missing\": 2}\n",
    "df_train[\"label\"] = df_train[\"type\"].map(label_map)\n",
    "\n",
    "print(\"\\nDistribusi awal label:\")\n",
    "print(df_train[\"label\"].value_counts())\n",
    "\n",
    "# ======================================\n",
    "# 2. Kurangi proporsi Missing (sampling)\n",
    "# ======================================\n",
    "missing_df = df_train[df_train[\"label\"] == 2]\n",
    "non_missing_df = df_train[df_train[\"label\"] != 2]\n",
    "\n",
    "# Ambil hanya 10% dari Missing\n",
    "missing_df_sample = missing_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Gabungkan kembali\n",
    "df_balanced = pd.concat([non_missing_df, missing_df_sample])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nDistribusi setelah sampling:\")\n",
    "print(df_balanced[\"label\"].value_counts())\n",
    "\n",
    "# Fitur dan target\n",
    "X_text = df_balanced[\"full_text\"]\n",
    "y = df_balanced[\"label\"]\n",
    "\n",
    "print(f\"\\nTotal data setelah filter: {len(df_balanced)}\")\n",
    "\n",
    "# ==== 3. Split Data ====\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_text, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==== 4. TF-IDF Vectorizer ====\n",
    "vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_val = vectorizer.transform(X_val_text)\n",
    "\n",
    "# ==== 5. XGBoost Training ====\n",
    "print(\"\\n🔄 Training XGBoost...\")\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",  # klasifikasi multi kelas\n",
    "    \"num_class\": len(label_map),  # 3 kelas\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=200,\n",
    "    evals=[(dval, \"Validation\")],\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# Evaluasi validasi\n",
    "val_preds = model.predict(dval)\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "print(f\"✅ Akurasi validasi: {val_acc:.4f}\")\n",
    "\n",
    "# ==== 6. Load Data Test ====\n",
    "df_test = pd.read_csv(\"parsed_articles_test_clean.csv\")\n",
    "df_test[\"full_text\"] = (df_test[\"abstract\"].fillna(\"\") + \" \" + df_test[\"body\"].fillna(\"\"))\n",
    "\n",
    "# Gabungkan dengan test_labels supaya dapat dataset_id\n",
    "test_labels = pd.read_csv(\"test_labels.csv\")\n",
    "df_test = df_test.merge(test_labels, on=\"article_id\", how=\"left\")\n",
    "\n",
    "print(\"\\nJumlah data test:\", len(df_test))\n",
    "\n",
    "# ==== 7. Evaluasi dengan 10% data test yang punya type ====\n",
    "if \"type\" in df_test.columns:\n",
    "    labeled_test = df_test.dropna(subset=[\"type\"])\n",
    "    print(f\"Ada {len(labeled_test)} data test yang punya label type.\")\n",
    "    \n",
    "    if len(labeled_test) > 0:\n",
    "        eval_sample = labeled_test.sample(frac=0.1, random_state=42)\n",
    "        X_eval = vectorizer.transform(eval_sample[\"full_text\"])\n",
    "        deval = xgb.DMatrix(X_eval)\n",
    "        eval_pred = model.predict(deval)\n",
    "        \n",
    "        # Map balik ke type\n",
    "        inv_label_map = {v: k for k, v in label_map.items()}\n",
    "        eval_pred_labels = [inv_label_map[int(p)] for p in eval_pred]\n",
    "        \n",
    "        print(f\"✅ Akurasi pada 10% data test berlabel: {accuracy_score(eval_sample['type'], eval_pred_labels):.4f}\")\n",
    "    else:\n",
    "        print(\"⚠ Tidak ada data test berlabel untuk evaluasi.\")\n",
    "else:\n",
    "    print(\"⚠ Kolom type tidak ditemukan di test_labels.\")\n",
    "\n",
    "# ==== 8. Prediksi seluruh test untuk submission ====\n",
    "X_test = vectorizer.transform(df_test[\"full_text\"])\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "test_preds = model.predict(dtest)\n",
    "\n",
    "# Map balik ke type\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "test_predictions = [inv_label_map[int(p)] for p in test_preds]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": range(1, len(df_test) + 1),\n",
    "    \"article_id\": df_test[\"article_id\"],\n",
    "    \"dataset_id\": df_test[\"dataset_id\"],  # dari test_labels\n",
    "    \"type\": test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_xgboost_balanced.csv\", index=False)\n",
    "print(\"\\n✅ File submission_xgboost_balanced.csv berhasil dibuat!\")\n",
    "print(submission.head())\n",
    "\n",
    "# ==== 9. Simpan Model dan Vectorizer ====\n",
    "joblib.dump((model, vectorizer), \"xgboost_text_model_balanced.pkl\")\n",
    "print(\"\\n✅ Model disimpan sebagai xgboost_text_model_balanced.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59fdbbe4-4227-4cbb-b204-5fecc3e26288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from cnn_tuning\\text_classification\\tuner0.json\n",
      "Best Hyperparameters:\n",
      "Embedding Dim: 300\n",
      "Filters: 256\n",
      "Kernel Size: 5\n",
      "Dropout: 0.3\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 3s 46ms/step - loss: 0.9040 - accuracy: 0.5243 - val_loss: 0.7414 - val_accuracy: 0.6796\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 0.6321 - accuracy: 0.7105 - val_loss: 0.6233 - val_accuracy: 0.7039\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 1s 27ms/step - loss: 0.5120 - accuracy: 0.7664 - val_loss: 0.5698 - val_accuracy: 0.7184\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 0.4584 - accuracy: 0.7737 - val_loss: 0.5511 - val_accuracy: 0.7282\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 0.4326 - accuracy: 0.7725 - val_loss: 0.5415 - val_accuracy: 0.7233\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 0.4232 - accuracy: 0.7737 - val_loss: 0.5373 - val_accuracy: 0.7184\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 1s 27ms/step - loss: 0.4139 - accuracy: 0.7713 - val_loss: 0.5378 - val_accuracy: 0.7184\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 0.4114 - accuracy: 0.7737 - val_loss: 0.5369 - val_accuracy: 0.7233\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 1s 27ms/step - loss: 0.4190 - accuracy: 0.7567 - val_loss: 0.5334 - val_accuracy: 0.7233\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 0.4113 - accuracy: 0.7725 - val_loss: 0.5355 - val_accuracy: 0.7184\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "25 25 25\n",
      "\n",
      "✅ File submission_cnn_fixed.csv berhasil dibuat!\n",
      "   row_id              article_id  \\\n",
      "0       1    10.1002_2017jc013030   \n",
      "1       2  10.1002_anie.201916483   \n",
      "2       3  10.1002_anie.202005531   \n",
      "3       4  10.1002_anie.202007717   \n",
      "4       5  10.1002_chem.201902131   \n",
      "\n",
      "                                          dataset_id     type  \n",
      "0                                            unknown  Primary  \n",
      "1                                            unknown  Missing  \n",
      "2                                            unknown  Primary  \n",
      "3  https://www.ccdc.cam.ac.uk/services/structures...  Missing  \n",
      "4                                            unknown  Missing  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "import keras_tuner as kt\n",
    "\n",
    "# ==== 1. Load & Preprocess Data ====\n",
    "df_articles = pd.read_csv(\"parsed_articles_train_clean.csv\")  # abstract + body\n",
    "df_labels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "# Gabungkan teks\n",
    "df = df_labels.merge(df_articles, on=\"article_id\", how=\"left\")\n",
    "df[\"full_text\"] = (df[\"abstract\"].fillna(\"\") + \" \" + df[\"body\"].fillna(\"\"))\n",
    "\n",
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"type\"])\n",
    "\n",
    "X_text = df[\"full_text\"].tolist()\n",
    "y = to_categorical(df[\"label\"], num_classes=len(le.classes_))\n",
    "\n",
    "# Split data\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "max_words = 30000\n",
    "max_len = 500\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=max_len)\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(X_val_text), maxlen=max_len)\n",
    "\n",
    "# ==== 2. Define CNN Model with Hyperparameters ====\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, hp.Int('embedding_dim', 100, 300, step=50), input_length=max_len))\n",
    "    model.add(Conv1D(filters=hp.Int('filters', 64, 256, step=64),\n",
    "                     kernel_size=hp.Choice('kernel_size', [3, 5, 7]),\n",
    "                     activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(hp.Float('dropout', 0.3, 0.6, step=0.1)))\n",
    "    model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ==== 3. Hyperparameter Tuning ====\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory='cnn_tuning',\n",
    "    project_name='text_classification'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=5,\n",
    "             validation_data=(X_val, y_val),\n",
    "             batch_size=32)\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Embedding Dim: {best_hp.get('embedding_dim')}\")\n",
    "print(f\"Filters: {best_hp.get('filters')}\")\n",
    "print(f\"Kernel Size: {best_hp.get('kernel_size')}\")\n",
    "print(f\"Dropout: {best_hp.get('dropout')}\")\n",
    "\n",
    "# ==== 4. Train Final Model ====\n",
    "model = tuner.hypermodel.build(best_hp)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=32)\n",
    "\n",
    "# ==== 5. Save Model ====\n",
    "model.save(\"cnn_text_classification.h5\")\n",
    "\n",
    "# ==== 6. Test Data Prediction ====\n",
    "test_text = pd.read_csv(\"parsed_articles_test_clean.csv\")\n",
    "test_labels = pd.read_csv(\"test_labels.csv\")  # ✅ berisi article_id + dataset_id\n",
    "test_text[\"full_text\"] = (test_text[\"abstract\"].fillna(\"\") + \" \" + test_text[\"body\"].fillna(\"\"))\n",
    "\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_text[\"full_text\"].tolist()), maxlen=max_len)\n",
    "pred_probs = model.predict(X_test)\n",
    "pred_classes = np.argmax(pred_probs, axis=1)\n",
    "pred_labels = le.inverse_transform(pred_classes)\n",
    "\n",
    "# ==== 7. Pastikan dataset_id sesuai ====\n",
    "id_map = dict(zip(test_labels['article_id'], test_labels['dataset_id']))\n",
    "dataset_ids = test_text['article_id'].map(id_map)\n",
    "\n",
    "print(len(test_text), len(pred_labels), len(dataset_ids))  # Debug: harus sama\n",
    "\n",
    "# ==== 8. Create Submission ====\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": range(1, len(test_text)+1),\n",
    "    \"article_id\": test_text[\"article_id\"],\n",
    "    \"dataset_id\": dataset_ids,\n",
    "    \"type\": pred_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_cnn_fixed.csv\", index=False)\n",
    "print(\"\\n✅ File submission_cnn_fixed.csv berhasil dibuat!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69c4bbb-1610-4a5c-8cc5-87061abf3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Variasi 10%: Total=638, Primary=195, Secondary=423, Missing=20\n",
      "\n",
      "📌 Variasi 30%: Total=677, Primary=195, Secondary=423, Missing=59\n",
      "\n",
      "📌 Variasi 50%: Total=716, Primary=195, Secondary=423, Missing=98\n",
      "\n",
      "📌 Variasi full: Total=815, Primary=195, Secondary=423, Missing=197\n",
      "\n",
      "===== 🔥 Training untuk variasi 10% =====\n",
      "✅ Generate BERT embeddings (train & val)...\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 36s 263ms/step - loss: 0.6389 - accuracy: 0.6973 - val_loss: 0.5007 - val_accuracy: 0.7760\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 26s 230ms/step - loss: 0.4084 - accuracy: 0.8206 - val_loss: 0.3608 - val_accuracy: 0.8490\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 26s 234ms/step - loss: 0.3078 - accuracy: 0.8655 - val_loss: 0.3060 - val_accuracy: 0.8542\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 0.2452 - accuracy: 0.8946 - val_loss: 0.3796 - val_accuracy: 0.8490\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 24s 216ms/step - loss: 0.2287 - accuracy: 0.8722 - val_loss: 0.3117 - val_accuracy: 0.8594\n",
      "6/6 [==============================] - 2s 91ms/step\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Primary       0.90      0.61      0.73        59\n",
      "   Secondary       0.85      1.00      0.92       127\n",
      "     Missing       0.67      0.33      0.44         6\n",
      "\n",
      "    accuracy                           0.86       192\n",
      "   macro avg       0.81      0.65      0.70       192\n",
      "weighted avg       0.86      0.86      0.85       192\n",
      "\n",
      "✅ Generate BERT embeddings (test)...\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "✅ Submission saved as submission_cnn_bilstm_bert_missing_10%.csv\n",
      "\n",
      "===== 🔥 Training untuk variasi 30% =====\n",
      "✅ Generate BERT embeddings (train & val)...\n",
      "Epoch 1/5\n",
      "119/119 [==============================] - 32s 220ms/step - loss: 0.6981 - accuracy: 0.7061 - val_loss: 0.4934 - val_accuracy: 0.7696\n",
      "Epoch 2/5\n",
      "119/119 [==============================] - 26s 219ms/step - loss: 0.4059 - accuracy: 0.8351 - val_loss: 0.3316 - val_accuracy: 0.8088\n",
      "Epoch 3/5\n",
      "119/119 [==============================] - 29s 243ms/step - loss: 0.2603 - accuracy: 0.8922 - val_loss: 0.3256 - val_accuracy: 0.8137\n",
      "Epoch 4/5\n",
      "119/119 [==============================] - 27s 229ms/step - loss: 0.2583 - accuracy: 0.8901 - val_loss: 0.3393 - val_accuracy: 0.8284\n",
      "Epoch 5/5\n",
      "119/119 [==============================] - 26s 222ms/step - loss: 0.2103 - accuracy: 0.8858 - val_loss: 0.3627 - val_accuracy: 0.8382\n",
      "WARNING:tensorflow:5 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001525370E8C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "7/7 [==============================] - 3s 107ms/step\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Primary       0.84      0.54      0.66        59\n",
      "   Secondary       0.83      1.00      0.91       127\n",
      "     Missing       0.92      0.67      0.77        18\n",
      "\n",
      "    accuracy                           0.84       204\n",
      "   macro avg       0.87      0.74      0.78       204\n",
      "weighted avg       0.84      0.84      0.82       204\n",
      "\n",
      "✅ Generate BERT embeddings (test)...\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "✅ Submission saved as submission_cnn_bilstm_bert_missing_30%.csv\n",
      "\n",
      "===== 🔥 Training untuk variasi 50% =====\n",
      "✅ Generate BERT embeddings (train & val)...\n",
      "Epoch 1/5\n",
      "126/126 [==============================] - 18s 71ms/step - loss: 0.7347 - accuracy: 0.6487 - val_loss: 0.5172 - val_accuracy: 0.7907\n",
      "Epoch 2/5\n",
      "126/126 [==============================] - 7s 52ms/step - loss: 0.3655 - accuracy: 0.8323 - val_loss: 0.4135 - val_accuracy: 0.8140\n",
      "Epoch 3/5\n",
      "126/126 [==============================] - 7s 55ms/step - loss: 0.2680 - accuracy: 0.8623 - val_loss: 0.3753 - val_accuracy: 0.8279\n",
      "Epoch 4/5\n",
      "126/126 [==============================] - 6s 50ms/step - loss: 0.1893 - accuracy: 0.9022 - val_loss: 0.5164 - val_accuracy: 0.8000\n",
      "Epoch 5/5\n",
      "126/126 [==============================] - 6s 50ms/step - loss: 0.2016 - accuracy: 0.8982 - val_loss: 0.3814 - val_accuracy: 0.8326\n",
      "7/7 [==============================] - 2s 51ms/step\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Primary       0.79      0.56      0.65        59\n",
      "   Secondary       0.83      0.96      0.89       127\n",
      "     Missing       0.92      0.83      0.87        29\n",
      "\n",
      "    accuracy                           0.83       215\n",
      "   macro avg       0.85      0.78      0.81       215\n",
      "weighted avg       0.83      0.83      0.82       215\n",
      "\n",
      "✅ Generate BERT embeddings (test)...\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "✅ Submission saved as submission_cnn_bilstm_bert_missing_50%.csv\n",
      "\n",
      "===== 🔥 Training untuk variasi full =====\n",
      "✅ Generate BERT embeddings (train & val)...\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 21s 79ms/step - loss: 0.7664 - accuracy: 0.6368 - val_loss: 0.3962 - val_accuracy: 0.8286\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 9s 61ms/step - loss: 0.4021 - accuracy: 0.8404 - val_loss: 0.3243 - val_accuracy: 0.8531\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 8s 55ms/step - loss: 0.2968 - accuracy: 0.8789 - val_loss: 0.2961 - val_accuracy: 0.8612\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.2339 - accuracy: 0.9070 - val_loss: 0.5550 - val_accuracy: 0.7633\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 7s 51ms/step - loss: 0.2135 - accuracy: 0.9070 - val_loss: 0.2818 - val_accuracy: 0.8694\n",
      "8/8 [==============================] - 2s 74ms/step\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Primary       0.83      0.58      0.68        59\n",
      "   Secondary       0.84      1.00      0.91       127\n",
      "     Missing       0.98      0.88      0.93        59\n",
      "\n",
      "    accuracy                           0.87       245\n",
      "   macro avg       0.88      0.82      0.84       245\n",
      "weighted avg       0.87      0.87      0.86       245\n",
      "\n",
      "✅ Generate BERT embeddings (test)...\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "✅ Submission saved as submission_cnn_bilstm_bert_missing_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ======================================\n",
    "# 1. Load Dataset\n",
    "# ======================================\n",
    "train_text = pd.read_csv(\"parsed_articles_train_clean.csv\")\n",
    "test_text = pd.read_csv(\"parsed_articles_test_clean.csv\")\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "test_labels = pd.read_csv(\"test_labels.csv\")\n",
    "\n",
    "df = train_labels.merge(train_text, on=\"article_id\")\n",
    "df.fillna(\"unknown\", inplace=True)\n",
    "\n",
    "# Mapping label\n",
    "label_map = {\"Primary\": 0, \"Secondary\": 1, \"Missing\": 2}\n",
    "reverse_map = {v: k for k, v in label_map.items()}\n",
    "df[\"label\"] = df[\"type\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "# ======================================\n",
    "# 2. Variasi Missing\n",
    "# ======================================\n",
    "missing_df = df[df[\"label\"] == 2]\n",
    "non_missing_df = df[df[\"label\"] != 2]\n",
    "\n",
    "ratios = {\"10%\": 0.10, \"30%\": 0.30, \"50%\": 0.50, \"full\": 1.00}\n",
    "datasets = {}\n",
    "\n",
    "for name, frac in ratios.items():\n",
    "    if frac < 1.0:\n",
    "        missing_sample = missing_df.sample(frac=frac, random_state=42)\n",
    "    else:\n",
    "        missing_sample = missing_df.copy()\n",
    "    \n",
    "    df_variant = pd.concat([non_missing_df, missing_sample])\n",
    "    df_variant = df_variant.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n📌 Variasi {name}: Total={len(df_variant)}, \"\n",
    "          f\"Primary={sum(df_variant.label==0)}, \"\n",
    "          f\"Secondary={sum(df_variant.label==1)}, \"\n",
    "          f\"Missing={sum(df_variant.label==2)}\")\n",
    "    \n",
    "    datasets[name] = df_variant\n",
    "\n",
    "# ======================================\n",
    "# 3. BERT Model\n",
    "# ======================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "MAX_LEN = 128\n",
    "\n",
    "def get_bert_embeddings(text_list):\n",
    "    embeddings = []\n",
    "    batch_size = 8\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state.cpu().numpy()\n",
    "        embeddings.append(cls_embeddings)\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# ======================================\n",
    "# 4. Training Loop & Submission\n",
    "# ======================================\n",
    "for name, df_variant in datasets.items():\n",
    "    print(f\"\\n===== 🔥 Training untuk variasi {name} =====\")\n",
    "\n",
    "    # Gabungkan text\n",
    "    df_variant[\"full_text\"] = df_variant[\"abstract\"].astype(str) + \" \" + df_variant[\"body\"].astype(str)\n",
    "\n",
    "    # Split data\n",
    "    train_df, val_df = train_test_split(df_variant, test_size=0.3, stratify=df_variant[\"label\"], random_state=42)\n",
    "\n",
    "    X_train_texts = train_df[\"full_text\"].tolist()\n",
    "    X_val_texts = val_df[\"full_text\"].tolist()\n",
    "    y_train = to_categorical(train_df[\"label\"].values, num_classes=3)\n",
    "    y_val = to_categorical(val_df[\"label\"].values, num_classes=3)\n",
    "\n",
    "    print(\"✅ Generate BERT embeddings (train & val)...\")\n",
    "    X_train = get_bert_embeddings(X_train_texts)\n",
    "    X_val = get_bert_embeddings(X_val_texts)\n",
    "\n",
    "    # ======================================\n",
    "    # 5. CNN + BiLSTM Model\n",
    "    # ======================================\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(MAX_LEN, 768)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=4, verbose=1)\n",
    "\n",
    "    # Evaluate\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_labels_pred = np.argmax(val_pred, axis=1)\n",
    "    val_labels_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "    print(\"\\n📊 Classification Report:\")\n",
    "    print(classification_report(val_labels_true, val_labels_pred, target_names=[\"Primary\", \"Secondary\", \"Missing\"]))\n",
    "\n",
    "    # ======================================\n",
    "    # 6. Predict Test & Save Submission\n",
    "    # ======================================\n",
    "    print(\"✅ Generate BERT embeddings (test)...\")\n",
    "    test_text[\"full_text\"] = test_text[\"abstract\"].astype(str) + \" \" + test_text[\"body\"].astype(str)\n",
    "    X_test = get_bert_embeddings(test_text[\"full_text\"].tolist())\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    pred_labels = np.argmax(preds, axis=1)\n",
    "    pred_types = [reverse_map[i] for i in pred_labels]\n",
    "\n",
    "    submission = test_text[[\"article_id\"]].copy()\n",
    "    submission.insert(0, \"row_id\", range(1, len(submission) + 1))\n",
    "    submission[\"type\"] = pred_types\n",
    "\n",
    "    submission = submission.merge(test_labels, on=\"article_id\", how=\"left\")\n",
    "    submission = submission[[\"row_id\", \"article_id\", \"dataset_id\", \"type\"]]\n",
    "\n",
    "    file_name = f\"submission_cnn_bilstm_bert_missing_{name}.csv\"\n",
    "    submission.to_csv(file_name, index=False)\n",
    "    print(f\"✅ Submission saved as {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "622aaaa0-7d76-406c-bd1c-0d49ffbd9d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ No dataset found in 10.1002_2017jc013030.xml\n",
      "⚠ No dataset found in 10.1002_anie.201916483.xml\n",
      "⚠ No dataset found in 10.1002_anie.202005531.xml\n",
      "✅ Found dataset for 10.1002_anie.202007717.xml: ['https://www.ccdc.cam.ac.uk/services/structures?id=doi:10.1002/anie.202007717']\n",
      "⚠ No dataset found in 10.1002_chem.201902131.xml\n",
      "✅ Found dataset for 10.1002_chem.201903120.xml: ['https://www.ccdc.cam.ac.uk/services/structures?id=doi:10.1002/chem.201903120']\n",
      "⚠ No dataset found in 10.1002_chem.202000235.xml\n",
      "⚠ No dataset found in 10.1002_chem.202001412.xml\n",
      "✅ Found dataset for 10.1002_chem.202001668.xml: ['https://www.ccdc.cam.ac.uk/services/structures?id=doi:10.1002/chem.202001668']\n",
      "✅ Found dataset for 10.1002_chem.202003167.xml: ['https://summary.ccdc.cam.ac.uk/structure-summary?doi=10.1002/chem.202003167']\n",
      "✅ Found dataset for 10.1002_cssc.202201821.xml: ['https://doi.org/10.5281/zenodo.7074790']\n",
      "✅ Found dataset for 10.1002_ece3.3985.xml: ['https://doi.org/10.1007/s12686-009-9039-2', 'https://doi.org/10.1534/genetics.166.4.1963', 'https://doi.org/10.1071/MF9740221', 'https://doi.org/10.1007/s12038-013-9392-x', 'https://doi.org/10.1163/156854094X00341', 'https://doi.org/10.1016/j.tree.2014.04.005', 'https://doi.org/10.1111/j.1469-7998.1990.tb04304.x', 'https://doi.org/10.1086/286006', 'https://doi.org/10.1111/j.1471-8286.2004.00684.x', 'https://doi.org/10.1371/journal.pone.0139585', 'https://doi.org/10.1007/s00265-006-0301-2', 'https://doi.org/10.1163/1937240X-00002254', 'https://doi.org/10.1007/s00227-006-0349-6', 'https://doi.org/10.1086/284170', 'https://doi.org/10.1163/193724012X626458', 'https://doi.org/10.1080/17451000.2010.528771', 'https://doi.org/10.2307/1548731', 'https://doi.org/10.3354/meps07634', 'https://doi.org/10.1111/j.1471-8286.2005.01029.x', 'https://doi.org/10.1086/285206', 'https://doi.org/10.2307/1548717', 'https://doi.org/10.1086/286101', 'https://doi.org/10.1111/j.1365-294X.2005.02498.x', 'https://doi.org/10.1007/s00265-015-2026-6', 'https://doi.org/10.1111/j.1365-2427.2007.01814.x', 'https://doi.org/10.1038/ncomms12452', 'https://doi.org/10.1111/j.1469-185X.1970.tb01176.x', 'https://doi.org/10.1016/j.icesjms.2006.06.006', 'https://doi.org/10.1371/journal.pone.0090680', 'https://doi.org/10.1071/MF9860055', 'https://doi.org/10.1007/s00265-012-1349-9', 'https://doi.org/10.1098/rspb.2000.1178', 'https://doi.org/10.1186/1471-2148-14-81', 'https://doi.org/10.1007/s00265-016-2240-x', 'https://doi.org/10.1046/j.1365-294x.2000.00964.x', 'https://doi.org/10.1046/j.1365-2427.1998.00355.x', 'https://doi.org/10.2307/1541301', 'https://doi.org/10.1093/oxfordjournals.jhered.a111573', 'https://doi.org/10.1038/nrg774', 'https://doi.org/10.1111/j.1471-8286.2005.01155.x', 'https://doi.org/10.1111/j.1471-8286.2006.01428.x', 'https://doi.org/10.1007/s00265-003-0677-1', 'https://doi.org/10.3354/meps08895', 'https://doi.org/10.1016/j.tree.2017.02.010', 'https://doi.org/10.1139/f98-006', 'https://doi.org/10.4159/harvard.9780674433960', 'https://doi.org/10.1071/MF9760499', 'https://doi.org/10.1534/genetics.108.100214', 'https://doi.org/10.1098/rstb.2012.0041', 'https://doi.org/10.1111/evo.13108', 'https://doi.org/10.2307/1549128', 'https://doi.org/10.1006/anbe.1994.1338', 'https://doi.org/10.1017/S0006323199005423', 'https://doi.org/10.1002/ece3.3985']\n",
      "✅ Found dataset for 10.1002_ece3.4466.xml: ['https://doi.org/10.5061/dryad.r6nq870']\n",
      "✅ Found dataset for 10.1002_ece3.5260.xml: ['https://doi.org/10.5061/dryad.2f62927']\n",
      "✅ Found dataset for 10.1002_ece3.5395.xml: ['https://doi.org/10.5441/001/1.v1cs4nn0', 'https://doi.org/10.5441/001/1.c42j3js7', 'https://doi.org/10.5441/001/1.4192t2j4', 'https://doi.org/10.5441/001/1.71r7pp6q', 'https://doi.org/10.5441/001/1.ck04mn78']\n",
      "✅ Found dataset for 10.1002_ece3.6144.xml: ['https://doi.org/10.5061/dryad.zw3r22854']\n",
      "✅ Found dataset for 10.1002_ece3.6303.xml: ['https://doi.org/10.5061/dryad.37pvmcvgb']\n",
      "⚠ No dataset found in 10.1002_ece3.6784.xml\n",
      "⚠ No dataset found in 10.1002_ece3.961.xml\n",
      "✅ Found dataset for 10.1002_ece3.9627.xml: ['https://doi.org/10.5061/dryad.b8gtht7h3', 'https://doi.org/10.6084/m9.figshare.21619055']\n",
      "⚠ No dataset found in 10.1002_ecs2.4619.xml\n",
      "⚠ No dataset found in 10.1002_ejic.201900904.xml\n",
      "✅ Found dataset for 10.1002_ejoc.202000916.xml: ['https://www.ccdc.cam.ac.uk/services/structures?id=doi:10.1002/ejoc.202000916']\n",
      "⚠ No dataset found in 10.1002_esp.5090.xml\n",
      "✅ Found dataset for 10.1002_mp.14424.xml: ['https://doi.org/10.7937/tcia.2020.6c7y-gq39']\n",
      "✅ Done! File saved as test_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Ambil dataset link lengkap ===\n",
    "def extract_dataset_links(root):\n",
    "    dataset_links = set()\n",
    "\n",
    "    # Cari <related-object content-type=\"dataset\">\n",
    "    for ro in root.findall(\".//related-object[@content-type='dataset']\"):\n",
    "        href = ro.get(\"{http://www.w3.org/1999/xlink}href\")\n",
    "        if href:\n",
    "            dataset_links.add(href.strip())\n",
    "\n",
    "    # Cari <ext-link> yang mungkin berisi dataset\n",
    "    for el in root.findall(\".//ext-link\"):\n",
    "        href = el.get(\"{http://www.w3.org/1999/xlink}href\")\n",
    "        if href and is_dataset_link(href):\n",
    "            dataset_links.add(href.strip())\n",
    "\n",
    "    return list(dataset_links)\n",
    "\n",
    "# === 2. Identifikasi apakah link dataset ===\n",
    "def is_dataset_link(url):\n",
    "    keywords = [\"figshare\", \"zenodo\", \"datadryad\", \"doi\"]\n",
    "    return any(kw in url.lower() for kw in keywords)\n",
    "\n",
    "# === 3. Path folder XML test ===\n",
    "xml_folder = \"test/XML\"\n",
    "parsed_labels = []\n",
    "\n",
    "# === 4. Proses semua file XML ===\n",
    "for fname in os.listdir(xml_folder):\n",
    "    if fname.endswith(\".xml\"):\n",
    "        full_path = os.path.join(xml_folder, fname)\n",
    "        try:\n",
    "            tree = ET.parse(full_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Ambil article_id\n",
    "            doi_elem = root.find(\".//article-id[@pub-id-type='doi']\")\n",
    "            article_id = doi_elem.text.strip().replace(\"/\", \"_\") if doi_elem is not None else fname.replace(\".xml\", \"\")\n",
    "\n",
    "            # Ambil dataset links (full DOI atau URL)\n",
    "            dataset_links = extract_dataset_links(root)\n",
    "            if dataset_links:\n",
    "                for ds_link in dataset_links:\n",
    "                    parsed_labels.append({\"article_id\": article_id, \"dataset_id\": ds_link})\n",
    "                print(f\"✅ Found dataset for {fname}: {dataset_links}\")\n",
    "            else:\n",
    "                parsed_labels.append({\"article_id\": article_id, \"dataset_id\": \"unknown\"})\n",
    "                print(f\"⚠ No dataset found in {fname}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error parsing {fname}: {e}\")\n",
    "\n",
    "# === 5. Simpan ke CSV ===\n",
    "df_labels = pd.DataFrame(parsed_labels)\n",
    "df_labels.to_csv(\"test_labels.csv\", index=False)\n",
    "print(\"✅ Done! File saved as test_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea5e127-aa39-441d-801c-2a88cbfa773c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# eda_analysis.py\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ======================================\n",
    "# 1. Load Dataset\n",
    "# ======================================\n",
    "train_text = pd.read_csv(\"parsed_articles_train_clean.csv\")\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "df = train_labels.merge(train_text, on=\"article_id\")\n",
    "df.fillna(\"unknown\", inplace=True)\n",
    "\n",
    "# Mapping label\n",
    "label_map = {\"Primary\": 0, \"Secondary\": 1, \"Missing\": 2}\n",
    "df[\"label\"] = df[\"type\"].map(label_map)\n",
    "\n",
    "# ======================================\n",
    "# 2. Distribusi Label\n",
    "# ======================================\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=\"type\", data=df, palette=\"viridis\")\n",
    "plt.title(\"Distribusi Label\")\n",
    "plt.ylabel(\"Jumlah Artikel\")\n",
    "plt.show()\n",
    "print(\"\\nDistribusi Label:\\n\", df[\"type\"].value_counts())\n",
    "\n",
    "# ======================================\n",
    "# 3. Panjang Teks\n",
    "# ======================================\n",
    "df[\"abstract_length\"] = df[\"abstract\"].astype(str).apply(lambda x: len(x.split()))\n",
    "df[\"body_length\"] = df[\"body\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.histplot(df[\"abstract_length\"], bins=50, color=\"blue\", alpha=0.6, label=\"Abstract\")\n",
    "sns.histplot(df[\"body_length\"], bins=50, color=\"orange\", alpha=0.6, label=\"Body\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribusi Panjang Abstract vs Body\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRata-rata Panjang per Label:\")\n",
    "print(df.groupby(\"type\")[[\"abstract_length\",\"body_length\"]].mean())\n",
    "\n",
    "# ======================================\n",
    "# 4. Missing Values\n",
    "# ======================================\n",
    "print(\"\\nJumlah Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# ======================================\n",
    "# 5. Korelasi Panjang dengan Label\n",
    "# ======================================\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x=\"type\", y=\"body_length\", data=df, palette=\"Set2\")\n",
    "plt.title(\"Distribusi Panjang Body Berdasarkan Label\")\n",
    "plt.show()\n",
    "\n",
    "# ======================================\n",
    "# 6. Word Cloud per Label\n",
    "# ======================================\n",
    "for label in df[\"type\"].unique():\n",
    "    text = \" \".join(df[df[\"type\"]==label][\"body\"].astype(str).tolist())\n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud - {label}\")\n",
    "    plt.show()\n",
    "\n",
    "# ======================================\n",
    "# 7. TF-IDF Top Words per Label\n",
    "# ======================================\n",
    "vectorizer = TfidfVectorizer(max_features=20, stop_words=\"english\")\n",
    "for label in df[\"type\"].unique():\n",
    "    texts = df[df[\"type\"]==label][\"body\"].astype(str)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    mean_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "    top_words = sorted(zip(feature_names, mean_tfidf), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(f\"\\nTop 10 words for {label}:\")\n",
    "    for word, score in top_words:\n",
    "        print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc907a1-66a7-4938-b7ed-386c7447b723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
