{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7d8cc0-37a8-4995-9d6e-57dfabecb7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.1002_cssc.202201821</td>\n",
       "      <td>https://doi.org/10.5281/zenodo.7074790</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.1002_ece3.4466</td>\n",
       "      <td>https://doi.org/10.5061/dryad.r6nq870</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.1002_ece3.5260</td>\n",
       "      <td>https://doi.org/10.5061/dryad.2f62927</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>https://doi.org/10.5061/dryad.zw3r22854</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id              article_id                               dataset_id  \\\n",
       "0       0    10.1002_2017jc013030           https://doi.org/10.17882/49388   \n",
       "1       1  10.1002_cssc.202201821   https://doi.org/10.5281/zenodo.7074790   \n",
       "2       2       10.1002_ece3.4466    https://doi.org/10.5061/dryad.r6nq870   \n",
       "3       3       10.1002_ece3.5260    https://doi.org/10.5061/dryad.2f62927   \n",
       "4       4       10.1002_ece3.6144  https://doi.org/10.5061/dryad.zw3r22854   \n",
       "\n",
       "      type  \n",
       "0  Primary  \n",
       "1  Primary  \n",
       "2  Primary  \n",
       "3  Primary  \n",
       "4  Primary  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_pd = 'sample_submission.csv'\n",
    "pd_df = pd.read_csv(file_pd)\n",
    "\n",
    "\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14a31fa-72f6-4f40-8ca8-ec9e41c3445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Parsing selesai. Data disimpan ke parsed_articles_train_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_from_element(elem, skip_tags=(\"xref\", \"table-wrap\", \"fig\", \"table\", \"label\")):\n",
    "    \"\"\"Ekstrak teks dari elemen XML, abaikan tag tertentu\"\"\"\n",
    "    texts = []\n",
    "    for sub in elem.iter():\n",
    "        if sub.tag not in skip_tags:\n",
    "            if sub.text:\n",
    "                texts.append(sub.text.strip())\n",
    "            if sub.tail:\n",
    "                texts.append(sub.tail.strip())\n",
    "    return \" \".join(texts)\n",
    "\n",
    "def parse_article_xml_clean(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Ambil article_id dari DOI\n",
    "        doi_elem = root.find(\".//article-id[@pub-id-type='doi']\")\n",
    "        article_id = doi_elem.text.strip().replace(\"/\", \"_\") if doi_elem is not None else os.path.basename(xml_path).replace(\".xml\", \"\")\n",
    "\n",
    "        # Ambil abstract\n",
    "        abstract_elem = root.find(\".//abstract\")\n",
    "        abstract = extract_text_from_element(abstract_elem) if abstract_elem is not None else \"\"\n",
    "\n",
    "        # Ambil body (tanpa xref, figure, table)\n",
    "        body_elem = root.find(\".//body\")\n",
    "        sections = []\n",
    "        if body_elem is not None:\n",
    "            for sec in body_elem.findall(\".//sec\"):\n",
    "                sec_text = extract_text_from_element(sec)\n",
    "                if sec_text:\n",
    "                    sections.append(sec_text)\n",
    "        body = \"\\n\\n\".join(sections)\n",
    "\n",
    "        return {\n",
    "            \"article_id\": article_id,\n",
    "            \"abstract\": abstract,\n",
    "            \"body\": body,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error in {xml_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ðŸ”§ Sesuaikan path ini jika perlu\n",
    "xml_folder = \"train/XML\"\n",
    "\n",
    "# Proses semua file .xml\n",
    "parsed_articles = []\n",
    "for fname in os.listdir(xml_folder):\n",
    "    if fname.endswith(\".xml\"):\n",
    "        full_path = os.path.join(xml_folder, fname)\n",
    "        article_data = parse_article_xml_clean(full_path)\n",
    "        if article_data:\n",
    "            parsed_articles.append(article_data)\n",
    "\n",
    "# Simpan ke CSV yang rapi\n",
    "df_xml = pd.DataFrame(parsed_articles)\n",
    "df_xml.to_csv(\"parsed_articles_train_clean.csv\", index=False)\n",
    "print(\"âœ… Parsing selesai. Data disimpan ke parsed_articles_train_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e161c2cc-3b96-4d32-9671-c98145d1bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Selesai! Semua teks dihapus. File: train_merged_no_text.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load hasil parsing artikel\n",
    "df_articles = pd.read_csv(\"parsed_articles_train_clean.csv\")\n",
    "\n",
    "# 2. Load label (relasi article_id ke dataset_id dan type)\n",
    "df_labels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "# 3. Gabungkan ke satu DataFrame berdasarkan article_id\n",
    "df_merged = df_labels.merge(df_articles, on=\"article_id\", how=\"left\")\n",
    "\n",
    "# 4. Drop kolom abstract, body, dan text jika ada\n",
    "df_merged = df_merged.drop(columns=[\"abstract\", \"body\"], errors=\"ignore\")\n",
    "if \"text\" in df_merged.columns:\n",
    "    df_merged = df_merged.drop(columns=[\"text\"])\n",
    "\n",
    "# 5. Simpan ke file CSV baru\n",
    "df_merged.to_csv(\"train_merged_no_text.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Selesai! Semua teks dihapus. File: train_merged_no_text.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a849fbd-1621-4801-a120-d3757d2d4352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Parsing selesai. Data disimpan ke parsed_articles_test_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_from_element(elem, skip_tags=(\"xref\", \"table-wrap\", \"fig\", \"table\", \"label\")):\n",
    "    \"\"\"Ekstrak teks dari elemen XML, abaikan tag tertentu\"\"\"\n",
    "    texts = []\n",
    "    for sub in elem.iter():\n",
    "        if sub.tag not in skip_tags:\n",
    "            if sub.text:\n",
    "                texts.append(sub.text.strip())\n",
    "            if sub.tail:\n",
    "                texts.append(sub.tail.strip())\n",
    "    return \" \".join(texts)\n",
    "\n",
    "def parse_article_xml_clean(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Ambil article_id dari DOI\n",
    "        doi_elem = root.find(\".//article-id[@pub-id-type='doi']\")\n",
    "        article_id = doi_elem.text.strip().replace(\"/\", \"_\") if doi_elem is not None else os.path.basename(xml_path).replace(\".xml\", \"\")\n",
    "\n",
    "        # Ambil abstract\n",
    "        abstract_elem = root.find(\".//abstract\")\n",
    "        abstract = extract_text_from_element(abstract_elem) if abstract_elem is not None else \"\"\n",
    "\n",
    "        # Ambil body (tanpa xref, figure, table)\n",
    "        body_elem = root.find(\".//body\")\n",
    "        sections = []\n",
    "        if body_elem is not None:\n",
    "            for sec in body_elem.findall(\".//sec\"):\n",
    "                sec_text = extract_text_from_element(sec)\n",
    "                if sec_text:\n",
    "                    sections.append(sec_text)\n",
    "        body = \"\\n\\n\".join(sections)\n",
    "\n",
    "        return {\n",
    "            \"article_id\": article_id,\n",
    "            \"abstract\": abstract,\n",
    "            \"body\": body,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error in {xml_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ðŸ”§ Sesuaikan path ini jika perlu\n",
    "xml_folder = \"test/XML\"\n",
    "\n",
    "# Proses semua file .xml\n",
    "parsed_articles = []\n",
    "for fname in os.listdir(xml_folder):\n",
    "    if fname.endswith(\".xml\"):\n",
    "        full_path = os.path.join(xml_folder, fname)\n",
    "        article_data = parse_article_xml_clean(full_path)\n",
    "        if article_data:\n",
    "            parsed_articles.append(article_data)\n",
    "\n",
    "# Simpan ke CSV yang rapi\n",
    "df_xml = pd.DataFrame(parsed_articles)\n",
    "df_xml.to_csv(\"parsed_articles_test_clean.csv\", index=False)\n",
    "print(\"âœ… Parsing selesai. Data disimpan ke parsed_articles_test_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eeb1426-a114-4e68-b2c7-8c14a215877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Selesai! Semua teks dihapus. File: test_merged_no_text.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load hasil parsing artikel\n",
    "df_articles = pd.read_csv(\"parsed_articles_test_clean.csv\")\n",
    "\n",
    "# 2. Load label (relasi article_id ke dataset_id dan type)\n",
    "df_labels = pd.read_csv(\"train_merged_no_text.csv\")\n",
    "\n",
    "# 3. Gabungkan ke satu DataFrame berdasarkan article_id\n",
    "df_merged = df_labels.merge(df_articles, on=\"article_id\", how=\"left\")\n",
    "\n",
    "# 4. Drop kolom abstract, body, dan text jika ada\n",
    "df_merged = df_merged.drop(columns=[\"abstract\", \"body\"], errors=\"ignore\")\n",
    "if \"text\" in df_merged.columns:\n",
    "    df_merged = df_merged.drop(columns=[\"text\"])\n",
    "\n",
    "# 5. Simpan ke file CSV baru\n",
    "df_merged.to_csv(\"test_merged_no_text.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Selesai! Semua teks dihapus. File: test_merged_no_text.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54376cd1-f421-421c-955c-164530851609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data train: 1028\n",
      "Distribusi label:\n",
      " type\n",
      "Secondary    449\n",
      "Missing      309\n",
      "Primary      270\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ”„ Training model...\n",
      "âœ… Akurasi validasi: 0.6408\n",
      "\n",
      "Jumlah data test: 25\n",
      "\n",
      "âœ… File submission.csv berhasil dibuat!\n",
      "   row_id              article_id dataset_id     type\n",
      "0       1    10.1002_2017jc013030    unknown  Primary\n",
      "1       2  10.1002_anie.201916483    unknown  Missing\n",
      "2       3  10.1002_anie.202005531    unknown  Primary\n",
      "3       4  10.1002_anie.202007717    unknown  Missing\n",
      "4       5  10.1002_chem.201902131    unknown  Missing\n",
      "\n",
      "âœ… Model disimpan sebagai text_classification_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# ==== 1. Load Data Train ====\n",
    "df_articles = pd.read_csv(\"parsed_articles_train_clean.csv\")  # kolom: article_id, abstract, body\n",
    "df_labels = pd.read_csv(\"train_labels.csv\")  # kolom: article_id, dataset_id, type\n",
    "\n",
    "# Gabungkan artikel dengan label\n",
    "df_train = df_labels.merge(df_articles, on=\"article_id\", how=\"left\")\n",
    "\n",
    "# Gabungkan abstract + body menjadi satu kolom teks\n",
    "df_train[\"full_text\"] = (df_train[\"abstract\"].fillna(\"\") + \" \" + df_train[\"body\"].fillna(\"\"))\n",
    "\n",
    "# Target dan fitur\n",
    "X_text = df_train[\"full_text\"]\n",
    "y = df_train[\"type\"]\n",
    "\n",
    "print(\"Jumlah data train:\", len(X_text))\n",
    "print(\"Distribusi label:\\n\", y.value_counts())\n",
    "\n",
    "# ==== 2. Split Data untuk Validasi ====\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==== 3. Pipeline TF-IDF + Logistic Regression ====\n",
    "model_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=30000, ngram_range=(1,2))),\n",
    "    ('clf', LogisticRegression(max_iter=200, solver='saga'))\n",
    "])\n",
    "\n",
    "# Latih model\n",
    "print(\"\\nðŸ”„ Training model...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluasi\n",
    "y_pred = model_pipeline.predict(X_val)\n",
    "print(f\"âœ… Akurasi validasi: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "# ==== 4. Prediksi untuk Data Test ====\n",
    "df_test = pd.read_csv(\"parsed_articles_test_clean.csv\")  # kolom: article_id, abstract, body\n",
    "df_test[\"full_text\"] = (df_test[\"abstract\"].fillna(\"\") + \" \" + df_test[\"body\"].fillna(\"\"))\n",
    "\n",
    "print(\"\\nJumlah data test:\", len(df_test))\n",
    "\n",
    "# Prediksi label test\n",
    "test_predictions = model_pipeline.predict(df_test[\"full_text\"])\n",
    "\n",
    "# ==== 5. Buat File Submission dengan tambahan row_id & dataset_id ====\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": range(1, len(df_test) + 1),  # mulai dari 1\n",
    "    \"article_id\": df_test[\"article_id\"],\n",
    "    \"dataset_id\": \"unknown\",  # karena tidak tersedia di test\n",
    "    \"type\": test_predictions\n",
    "})\n",
    "\n",
    "# Simpan file submission\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nâœ… File submission.csv berhasil dibuat!\")\n",
    "print(submission.head())\n",
    "\n",
    "# ==== 6. (Opsional) Simpan Model ====\n",
    "joblib.dump(model_pipeline, \"text_classification_model.pkl\")\n",
    "print(\"\\nâœ… Model disimpan sebagai text_classification_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fdbbe4-4227-4cbb-b204-5fecc3e26288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 13s]\n",
      "val_accuracy: 0.7135922312736511\n",
      "\n",
      "Best val_accuracy So Far: 0.7281553149223328\n",
      "Total elapsed time: 00h 01m 52s\n",
      "Best Hyperparameters:\n",
      "Embedding Dim: 300\n",
      "Filters: 256\n",
      "Kernel Size: 5\n",
      "Dropout: 0.3\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 9s 325ms/step - loss: 0.8901 - accuracy: 0.5061 - val_loss: 0.7404 - val_accuracy: 0.6845\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 5s 185ms/step - loss: 0.6229 - accuracy: 0.7202 - val_loss: 0.6163 - val_accuracy: 0.6893\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 4s 152ms/step - loss: 0.5059 - accuracy: 0.7676 - val_loss: 0.5736 - val_accuracy: 0.7184\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 4s 141ms/step - loss: 0.4563 - accuracy: 0.7701 - val_loss: 0.5495 - val_accuracy: 0.7282\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 4s 156ms/step - loss: 0.4342 - accuracy: 0.7713 - val_loss: 0.5445 - val_accuracy: 0.7136\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 4s 146ms/step - loss: 0.4213 - accuracy: 0.7713 - val_loss: 0.5466 - val_accuracy: 0.7136\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 9s 336ms/step - loss: 0.4153 - accuracy: 0.7725 - val_loss: 0.5417 - val_accuracy: 0.7233\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 4s 134ms/step - loss: 0.4121 - accuracy: 0.7725 - val_loss: 0.5434 - val_accuracy: 0.7184\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 9s 335ms/step - loss: 0.4133 - accuracy: 0.7737 - val_loss: 0.5448 - val_accuracy: 0.7039\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 9s 333ms/step - loss: 0.4093 - accuracy: 0.7762 - val_loss: 0.5410 - val_accuracy: 0.7136\n",
      "1/1 [==============================] - 0s 298ms/step\n",
      "\n",
      "âœ… File submission_cnn.csv berhasil dibuat!\n",
      "   row_id              article_id dataset_id     type\n",
      "0       1    10.1002_2017jc013030    unknown  Primary\n",
      "1       2  10.1002_anie.201916483    unknown  Missing\n",
      "2       3  10.1002_anie.202005531    unknown  Primary\n",
      "3       4  10.1002_anie.202007717    unknown  Missing\n",
      "4       5  10.1002_chem.201902131    unknown  Missing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "import keras_tuner as kt\n",
    "\n",
    "# ==== 1. Load & Preprocess Data ====\n",
    "df_articles = pd.read_csv(\"parsed_articles_train_clean.csv\")  # abstract + body\n",
    "df_labels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "# Gabungkan teks\n",
    "df = df_labels.merge(df_articles, on=\"article_id\", how=\"left\")\n",
    "df[\"full_text\"] = (df[\"abstract\"].fillna(\"\") + \" \" + df[\"body\"].fillna(\"\"))\n",
    "\n",
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"type\"])\n",
    "\n",
    "X_text = df[\"full_text\"].tolist()\n",
    "y = to_categorical(df[\"label\"], num_classes=len(le.classes_))\n",
    "\n",
    "# Split data\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(X_text, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Tokenizer\n",
    "max_words = 30000\n",
    "max_len = 500\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=max_len)\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(X_val_text), maxlen=max_len)\n",
    "\n",
    "# ==== 2. Define CNN Model with Hyperparameters ====\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, hp.Int('embedding_dim', 100, 300, step=50), input_length=max_len))\n",
    "    model.add(Conv1D(filters=hp.Int('filters', 64, 256, step=64),\n",
    "                     kernel_size=hp.Choice('kernel_size', [3, 5, 7]),\n",
    "                     activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(hp.Float('dropout', 0.3, 0.6, step=0.1)))\n",
    "    model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ==== 3. Hyperparameter Tuning ====\n",
    "tuner = kt.RandomSearch(build_model,\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=5,\n",
    "                        executions_per_trial=1,\n",
    "                        directory='cnn_tuning',\n",
    "                        project_name='text_classification')\n",
    "\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=5,\n",
    "             validation_data=(X_val, y_val),\n",
    "             batch_size=32)\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Embedding Dim: {best_hp.get('embedding_dim')}\")\n",
    "print(f\"Filters: {best_hp.get('filters')}\")\n",
    "print(f\"Kernel Size: {best_hp.get('kernel_size')}\")\n",
    "print(f\"Dropout: {best_hp.get('dropout')}\")\n",
    "\n",
    "# ==== 4. Train Final Model ====\n",
    "model = tuner.hypermodel.build(best_hp)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=32)\n",
    "\n",
    "# ==== 5. Save Model ====\n",
    "model.save(\"cnn_text_classification.h5\")\n",
    "\n",
    "# ==== 6. Test Data Prediction ====\n",
    "df_test = pd.read_csv(\"parsed_articles_test_clean.csv\")\n",
    "df_test[\"full_text\"] = (df_test[\"abstract\"].fillna(\"\") + \" \" + df_test[\"body\"].fillna(\"\"))\n",
    "\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(df_test[\"full_text\"].tolist()), maxlen=max_len)\n",
    "pred_probs = model.predict(X_test)\n",
    "pred_labels = le.inverse_transform(np.argmax(pred_probs, axis=1))\n",
    "\n",
    "# ==== 7. Create Submission ====\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": range(1, len(df_test)+1),\n",
    "    \"article_id\": df_test[\"article_id\"],\n",
    "    \"dataset_id\": \"unknown\",\n",
    "    \"type\": pred_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_cnn.csv\", index=False)\n",
    "print(\"\\nâœ… File submission_cnn.csv berhasil dibuat!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22cf9eb-13bd-46dc-b2a2-6e9ff0bd5410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (652, 6), Val: (163, 6)\n",
      "BERT embeddings shape (train): (652, 64, 768)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 62, 128)           295040    \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 31, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 402,307\n",
      "Trainable params: 402,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 16s 59ms/step - loss: 0.7591 - accuracy: 0.6472 - val_loss: 0.4853 - val_accuracy: 0.7485\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 6s 38ms/step - loss: 0.3670 - accuracy: 0.8436 - val_loss: 0.4235 - val_accuracy: 0.8466\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 0.3025 - accuracy: 0.8528 - val_loss: 0.4634 - val_accuracy: 0.8221\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 6s 39ms/step - loss: 0.2254 - accuracy: 0.8942 - val_loss: 0.5149 - val_accuracy: 0.8282\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 6s 35ms/step - loss: 0.2230 - accuracy: 0.8896 - val_loss: 0.4857 - val_accuracy: 0.8344\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 5s 31ms/step - loss: 0.1991 - accuracy: 0.8942 - val_loss: 0.5972 - val_accuracy: 0.8282\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 6s 36ms/step - loss: 0.1790 - accuracy: 0.9080 - val_loss: 0.5634 - val_accuracy: 0.8344\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 5s 33ms/step - loss: 0.1671 - accuracy: 0.9095 - val_loss: 0.5730 - val_accuracy: 0.8282\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 6s 36ms/step - loss: 0.1713 - accuracy: 0.9141 - val_loss: 0.6433 - val_accuracy: 0.8221\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 6s 37ms/step - loss: 0.1694 - accuracy: 0.9110 - val_loss: 0.6472 - val_accuracy: 0.8282\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "âœ… File submission_cnn_bilstm_bert.csv berhasil dibuat!\n",
      "   row_id              article_id     type\n",
      "0       1    10.1002_2017jc013030  Missing\n",
      "1       2  10.1002_anie.201916483  Missing\n",
      "2       3  10.1002_anie.202005531  Missing\n",
      "3       4  10.1002_anie.202007717  Missing\n",
      "4       5  10.1002_chem.201902131  Missing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ======================================\n",
    "# 1. Load Dataset\n",
    "# ======================================\n",
    "train_text = pd.read_csv(\"parsed_articles_train_clean.csv\")\n",
    "test_text = pd.read_csv(\"parsed_articles_test_clean.csv\")\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "# Gabungkan teks dengan label\n",
    "df = train_labels.merge(train_text, on=\"article_id\")\n",
    "df.fillna(\"unknown\", inplace=True)\n",
    "\n",
    "# Mapping label ke angka\n",
    "label_map = {\"Primary\": 0, \"Secondary\": 1, \"Missing\": 2}\n",
    "df[\"label\"] = df[\"type\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "# ======================================\n",
    "# 2. Train-Validation Split\n",
    "# ======================================\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "print(f\"Train: {train_df.shape}, Val: {val_df.shape}\")\n",
    "\n",
    "# ======================================\n",
    "# 3. BERT Tokenizer & Embedding\n",
    "# ======================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "MAX_LEN = 64  # gunakan max_len yg lebih besar utk hasil lebih stabil\n",
    "\n",
    "def get_bert_embeddings(text_list):\n",
    "    embeddings = []\n",
    "    batch_size = 8\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Gabungkan abstract + body\n",
    "for df_ in [train_df, val_df, test_text]:\n",
    "    df_[\"full_text\"] = df_[\"abstract\"].astype(str) + \" \" + df_[\"body\"].astype(str)\n",
    "\n",
    "train_texts = train_df[\"full_text\"].tolist()\n",
    "val_texts = val_df[\"full_text\"].tolist()\n",
    "test_texts = test_text[\"full_text\"].tolist()\n",
    "\n",
    "# Proses BERT embedding\n",
    "X_train = get_bert_embeddings(train_texts)\n",
    "X_val = get_bert_embeddings(val_texts)\n",
    "X_test = get_bert_embeddings(test_texts)\n",
    "\n",
    "y_train = to_categorical(train_df[\"label\"].values, num_classes=3)\n",
    "y_val = to_categorical(val_df[\"label\"].values, num_classes=3)\n",
    "\n",
    "print(f\"BERT embeddings shape (train): {X_train.shape}\")\n",
    "\n",
    "# ======================================\n",
    "# 4. Build CNN-BiLSTM Model\n",
    "# ======================================\n",
    "model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(MAX_LEN, 768)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ======================================\n",
    "# 5. Train the Model\n",
    "# ======================================\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=4)\n",
    "\n",
    "# ======================================\n",
    "# 6. Predict & Submission\n",
    "# ======================================\n",
    "preds = model.predict(X_test)\n",
    "pred_labels = np.argmax(preds, axis=1)\n",
    "\n",
    "reverse_map = {v: k for k, v in label_map.items()}\n",
    "pred_types = [reverse_map[i] for i in pred_labels]\n",
    "\n",
    "submission = test_text[[\"article_id\"]].copy()\n",
    "submission.insert(0, \"row_id\", range(1, len(submission) + 1))\n",
    "submission[\"type\"] = pred_types\n",
    "\n",
    "submission.to_csv(\"submission_cnn_bilstm_bert.csv\", index=False)\n",
    "print(\"âœ… File submission_cnn_bilstm_bert.csv berhasil dibuat!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c4bbb-1610-4a5c-8cc5-87061abf3700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
